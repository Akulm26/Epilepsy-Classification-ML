---
title: "Independent Research - Epilepsy"
author: "Sobanaa Jayakumar"
date: "6/16/2021"
output: html_document
---

```{r libraries}
library(corrplot)
library(tidyverse)
library(imbalance)
library(ROSE)
library(klaR) 
library(car)
library(glmnet)
library(pls)
library(ROCR)
library(FNN)
library(class)
```


```{r data loading}
df = read.csv("data.csv", header = T)
head(df)
# Dropping hashed Id column
df <- subset(df, select = -c(X))
df[, -179] <- scale(df[, -179])
head(df)

```


```{r summary stats}
#summary(df) - remove the hashtag for displaying summary stats 
```


```{r corrplot - NO CORRPLOT IS REQUIRED - SEE PYTHON SCRIPT}
corr_matrix = df[,2:179]
#corr_matrix
M <- cor(corr_matrix)
M

```



```{r NO CORRELATION OS REQUIRED - SEE PYTHON SCRIPT}
# corr_simple <- function(data = corr_matrix,sig= 0.1){
#   #convert data to numeric in order to run correlations
#   #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
#   df_cor <- data %>% mutate_if(is.character, as.factor)
#   df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
#   #run a correlation and drop the insignificant ones
#   corr <- cor(df_cor)
#   #prepare to drop duplicates and correlations of 1     
#   corr[lower.tri(corr,diag=TRUE)] <- NA 
#   #drop perfect correlations
#   corr[corr == 1] <- NA 
#   #turn into a 3-column table
#   corr <- as.data.frame(as.table(corr))
#   #remove the NA values from above 
#   corr <- na.omit(corr) 
#   #select significant values  
#   corr <- subset(corr, abs(Freq) > sig) 
#   #sort by highest correlation
#   corr <- corr[order(-abs(corr$Freq)),] 
#   #print table
#   print(corr)
#   #turn corr back into matrix in order to plot with corrplot
#   mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
#   
#   #plot correlations visually
#   #corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
# }
# corr_simple()


```


```{r Data Pre Preprocessing and Splitting}
# Conversion to binary
class(df$y)
df$y <- as.numeric(df$y)
class(df$y)
df <-  df %>% mutate(y = ifelse(y  < 2, 1, 0))
#df <- lapply(df,as.numeric)
df$y <- as.factor(df$y)
class(df$y)
head(df)
```


```{r Prevalence calculation}
#Prevalence calculation
#It is defined as the the percentage of samples belonging to the positive class. This means that, in our case, the prevalence value will depict the percentage of patients experiencing seizure. 

class(df$y)
y<-as.integer(df$y)
y<-y-1
sum(y)
length(y)

prevalence_calc<-function(df,y){
  y<-as.integer(df$y)
  y<-y-1
  sum(y)
  length(y)
  X = sum(y)/length(y)
  return(X)

}
prevalence_calc(df, y)

#Separating response from independent variables

input_df<-subset(df, select=X1:X178)
input_df <- scale(input_df)
head(input_df)
output_df<-subset(df, select = y)
head(output_df)

#Random shuffling of the original data
set.seed(1)
rows<-sample(nrow(df))
df_shuffled<-df[rows,]
df_shuffled


#Splitting data into train,test and validation

spec = c(train = .7, test = .15, validate = .15)

g = sample(cut(
  seq(nrow(df_shuffled)), 
  nrow(df_shuffled)*cumsum(c(0,spec)),
  labels = names(spec)
))

res = split(df_shuffled, g)
sapply(res, nrow)/nrow(df_shuffled)
# res$train
# res$test
# res$validate

prevalence_calc(res$train, y)
prevalence_calc(res$test, y)
prevalence_calc(res$validate, y)

#Encountering class imbalance issue

#oversampling in training data
#library ROSE and imbalance
#res$train[1:178] <- lapply(res$train[1:178], as.numeric)
train<-res$train
table(train$y)
prop.table(table(train$y))
n_legit<-6452
new_frac_legit<-0.50
new_n_total<- n_legit/new_frac_legit

oversampling_train_result<-ovun.sample(y~., data = train, method = "over", N = new_n_total, seed = 2018)
oversampled_train<-oversampling_train_result$data
table(oversampled_train$y)
prevalence_calc(oversampled_train,y)
dim(oversampled_train)

#Oversampling in test data
test<-res$test
table(test$y)
prop.table(table(test$y))
n_legit<-1348
new_frac_legit<-0.50
new_n_total<- n_legit/new_frac_legit

oversampling_test_result<-ovun.sample(y~., data = test, method = "over", N = new_n_total, seed = 2018)
oversampled_test<-oversampling_test_result$data
table(oversampled_test$y)
prevalence_calc(oversampled_test,y)
dim(oversampled_test)


#Oversampling in validate data

validate<-res$validate
table(validate$y)
prop.table(table(validate$y))
n_legit<-1400
new_frac_legit<-0.50
new_n_total<- n_legit/new_frac_legit

oversampling_validate_result<-ovun.sample(y~., data = validate, method = "over", N = new_n_total, seed = 2018)
oversampled_validate<-oversampling_validate_result$data
table(oversampled_validate$y)
prevalence_calc(oversampled_validate,y)
dim(oversampled_validate)

```

We'll have to segregate train into x_train and y_train and like this for other sets too i.e. test and validate. Post splitting, we need to standardize our feature inputs for removing high variance in data. ONce this is done, we're ready to deploy our classification algos. 

Multicollinearity: We cant remove features! There's no point checking since no frame readings can be removed.  

```{r train test and validate}
dim(oversampled_train)
table(oversampled_train$y)
dim(oversampled_test)
table(oversampled_test$y)
dim(oversampled_validate)
table(oversampled_validate$y)
```


```{r NAIVE BAYES}

library(naivebayes)
library(dplyr)
library(ggplot2)
library(caret)

model <- naive_bayes(y ~ ., data = oversampled_train, usekernel = T) 
#model
y_pred <- predict(model, newdata = oversampled_validate)
cm <- table(oversampled_validate$y, y_pred)
cm
confusionMatrix(cm)
```




